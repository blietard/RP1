{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from mangoes.modeling import PretrainedTransformerModelForFeatureExtraction as PreTrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] successfully constructed ELMo embedder\n"
     ]
    }
   ],
   "source": [
    "bert = PreTrained.load(\"bert-base-uncased\", \"bert-base-uncased\")\n",
    "print(\"[INFO] successfully constructed ELMo embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device :  GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "print('Device : ',torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I read an article last week : turns out it was a fake article !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = bert.generate_outputs(text, output_hidden_states=True, output_attentions=False, word_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'offset_mappings'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['hidden_states'][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "tensors = []\n",
    "for tensor in outputs['hidden_states'][-4:]:\n",
    "    tensors.append(tensor[0,ind])\n",
    "avg = torch.mean(torch.stack(tensors),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recleaner(noun):\n",
    "    '''\n",
    "    For a given noun, clean his 'not fake' file to make length matching with locations file\n",
    "    '''\n",
    "    print('Start for {}'.format(noun))\n",
    "    errors = 0\n",
    "    with open('cleaned_data/not_fake_'+noun+'.txt','r',encoding='utf-8') as dataTxt , \\\n",
    "         open('locationsFiles/loc_not_fake_'+noun+'.txt','r',encoding='utf-8') as locTxt :\n",
    "        data = dataTxt.read().split('\\n')\n",
    "        locations = locTxt.read().split('\\n')\n",
    "    data.pop(-1) #get rid of '' at the end\n",
    "    locations.pop(-1) #get rid of '' at the end\n",
    "    locList = []\n",
    "    for loc in locations: #several occurences in the same sentences\n",
    "        loc = loc.split('|')\n",
    "        loc = [int(x) for x in loc]\n",
    "        locList.append(loc)\n",
    "    #cleaning\n",
    "    N = len(data)\n",
    "    queue = data.copy()\n",
    "    keep = []\n",
    "    i=0\n",
    "    while queue:\n",
    "        success = True\n",
    "        sent = queue.pop(0)\n",
    "        loc = locList[i]\n",
    "        for n in loc:\n",
    "            if n>=len(sent.split()):\n",
    "                success = False\n",
    "            else:\n",
    "                if sent.split()[n] != noun:\n",
    "                    success = False  \n",
    "        if success:\n",
    "            keep.append(sent)\n",
    "            i+=1\n",
    "        else:\n",
    "            errors+=1\n",
    "    #Put in file\n",
    "    if len(locList)==len(keep):\n",
    "        with open('cleaned_data/not_fake_'+noun+'_recleaned.txt','w',encoding='utf-8') as dataTxt:\n",
    "            while keep:\n",
    "                sentence = keep.pop(0)\n",
    "                dataTxt.write(sentence+'\\n')\n",
    "    else:\n",
    "        raise ValueError('Unknow error for '+noun)\n",
    "    print('Finished for {} with {} deleted sentences'.format(noun,str(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(noun):\n",
    "    #============================Embeddings of FAKE and (fake) NOUN ============================\n",
    "    #File opening -------------------------------------------------------------------\n",
    "    print(\"[START] starting work on \" + noun + \"...\")\n",
    "    with open('cleaned_data/fake_'+noun+'.txt','r',encoding='utf-8') as dataTxt , \\\n",
    "         open('locationsFiles/loc_fake_'+noun+'.txt','r',encoding='utf-8') as locTxt :\n",
    "        data = dataTxt.read().split('\\n')\n",
    "        locations = locTxt.read().split('\\n')\n",
    "    #Data preparation -------------------------------------------------------------------\n",
    "    data.pop(-1) #get rid of '' at the end\n",
    "    locations.pop(-1) #get rid of '' at the end\n",
    "    locList = []\n",
    "    for loc in locations: #several occurences in the same sentences\n",
    "        loc = loc.split('|')\n",
    "        loc = [int(x) for x in loc]\n",
    "        locList.append(loc)\n",
    "    if len(locList)!=len(data):\n",
    "        print(\"[ERROR] non matching files for (fake)\" + noun + \"...\")\n",
    "        return\n",
    "    #Sampling\n",
    "    N = len(data)\n",
    "    sample = np.random.choice(N,N//10)\n",
    "    print(\"[FLW] data for (fake) \"+noun+\" successfully prepared. \")\n",
    "    #Embeddings creation -------------------------------------------------------------------\n",
    "    fakes = []\n",
    "    nouns = []\n",
    "    N = len(data)\n",
    "    fails = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        success = True\n",
    "        sent = data[i]\n",
    "        loc = locList[i]\n",
    "        if len(sent.split())>512: #too long sequence\n",
    "            fails+=len(loc)\n",
    "        else:\n",
    "            try:\n",
    "                hidden_states = bert.generate_outputs(sent, output_hidden_states=True, output_attentions=False, word_embeddings=True)['hidden_states'][-4:]\n",
    "            except RuntimeError:\n",
    "                success= False\n",
    "                fails+=len(loc)\n",
    "            if success:\n",
    "                for n in loc:\n",
    "                    tensors = []\n",
    "                    for tensor in hidden_states:\n",
    "                        tensors.append(tensor[0,n])\n",
    "                    fakeEmb = torch.mean(torch.stack(tensors),dim=0).numpy()\n",
    "                    tensors = []\n",
    "                    for tensor in hidden_states:\n",
    "                        tensors.append(tensor[0,n])\n",
    "                    nounEmb = torch.mean(torch.stack(tensors),dim=0).numpy()\n",
    "\n",
    "                    fakes.append(fakeEmb)\n",
    "                    nouns.append(nounEmb)\n",
    "        print(\"\\r[FLW] Embeddings construction (FAKE and fake NOUN) for {} : {}%\".format(noun,np.round((i+1)/N*100),4), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    print(\"\\r[FLW] Embeddings (FAKE and fake NOUN) constructed for {}  (lost embeddings : {})    \".format(noun,fails))\n",
    "    \n",
    "    with open('BERTfiles/FAKE'+ noun+'FAKE.npy','wb') as f:\n",
    "        np.save(file=f,arr=fakes)\n",
    "    with open('BERTfiles/'+noun+'FAKE.npy','wb') as f:\n",
    "        np.save(file=f,arr=nouns)\n",
    "        \n",
    "    \n",
    "    #============================ Embeddings of (not fake) NOUN ============================\n",
    "    \n",
    "    #File opening -------------------------------------------------------------------\n",
    "    with open('cleaned_data/not_fake_'+noun+'_recleaned.txt','r',encoding='utf-8') as dataTxt , \\\n",
    "         open('locationsFiles/loc_not_fake_'+noun+'.txt','r',encoding='utf-8') as locTxt :\n",
    "        data = dataTxt.read().split('\\n')\n",
    "        locations = locTxt.read().split('\\n')\n",
    "    #Data preparation -------------------------------------------------------------------\n",
    "    data.pop(-1) #get rid of '' at the end\n",
    "    locations.pop(-1) #get rid of '' at the end\n",
    "    locList = []\n",
    "    for loc in locations: #several occurences in the same sentences\n",
    "        loc = loc.split('|')\n",
    "        loc = [int(x) for x in loc]\n",
    "        locList.append(loc)\n",
    "    if len(locList)!=len(data):\n",
    "        print(\"[ERROR] non matching files for (fake)\" + noun + \"...\")\n",
    "        return\n",
    "    #Sampling\n",
    "    N = len(data)\n",
    "    sample = np.random.choice(N,N//10)\n",
    "    print(\"[FLW] data for (not fake) \"+noun+\" successfully prepared. \")\n",
    "    \n",
    "    nouns = []\n",
    "    N = len(data)\n",
    "    sample = np.random.choice(N,N//10)\n",
    "    fails = 0\n",
    "    \n",
    "    for j in range(len(sample)):\n",
    "        i = sample[j]\n",
    "        success = True\n",
    "        sent = data[i]\n",
    "        loc = locList[i]\n",
    "        if len(sent.split())>512: #too long sequence\n",
    "            fails+=len(loc)\n",
    "        else:\n",
    "            try:\n",
    "                hidden_states = bert.generate_outputs(sent, output_hidden_states=True, output_attentions=False, word_embeddings=True)['hidden_states'][-4:]\n",
    "            except RuntimeError:\n",
    "                success= False\n",
    "                fails+=1\n",
    "            if success:\n",
    "                for n in loc:\n",
    "                    tensors = []\n",
    "                    if n>hidden_states[-1].shape[1]:\n",
    "                        print('[ALERT ERROR] with')\n",
    "                        print(sent)\n",
    "                        print(loc,n)\n",
    "                        print(hidden_states[-1].shape)\n",
    "                        raise IndexError('Something wrong happened')\n",
    "                    for tensor in hidden_states:\n",
    "                        tensors.append(tensor[0,n])\n",
    "                    nounEmb = torch.mean(torch.stack(tensors),dim=0).numpy()\n",
    "                    nouns.append(nounEmb)\n",
    "        print(\"\\r[FLW] Embeddings construction (not fake NOUN) for {} : {}%\".format(noun,np.round((j+1)/(N//10)*100),4), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    print(\"\\r[FLW] Embeddings (not fake NOUN) constructed for {}  (lost embeddings : {})    \".format(noun,fails))\n",
    "    \n",
    "    with open('BERTfiles/'+noun+'.npy','wb') as f:\n",
    "        np.save(file=f,arr=nouns)\n",
    "    print(\"[SUCCESS] successfully created embeddings for \" + noun +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "listNouns = ['article','beard','blood','company','death','gun','id','interview','passport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start for article\n",
      "Finished for article with 567 deleted sentences\n",
      "Start for beard\n",
      "Finished for beard with 0 deleted sentences\n",
      "Start for blood\n",
      "Finished for blood with 0 deleted sentences\n",
      "Start for company\n",
      "Finished for company with 10 deleted sentences\n",
      "Start for death\n",
      "Finished for death with 2 deleted sentences\n",
      "Start for gun\n",
      "Finished for gun with 0 deleted sentences\n",
      "Start for id\n",
      "Finished for id with 0 deleted sentences\n",
      "Start for interview\n",
      "Finished for interview with 0 deleted sentences\n",
      "Start for passport\n",
      "Finished for passport with 0 deleted sentences\n"
     ]
    }
   ],
   "source": [
    "for noun in listNouns:\n",
    "    recleaner(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] starting work on article...\n",
      "[FLW] data for (fake) article successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for article  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) article successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for article  (lost embeddings : 28)    \n",
      "[SUCCESS] successfully created embeddings for article\n",
      "\n",
      "[START] starting work on beard...\n",
      "[FLW] data for (fake) beard successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for beard  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) beard successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for beard  (lost embeddings : 0)    \n",
      "[SUCCESS] successfully created embeddings for beard\n",
      "\n",
      "[START] starting work on blood...\n",
      "[FLW] data for (fake) blood successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for blood  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) blood successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for blood  (lost embeddings : 9)    \n",
      "[SUCCESS] successfully created embeddings for blood\n",
      "\n",
      "[START] starting work on company...\n",
      "[FLW] data for (fake) company successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for company  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) company successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for company  (lost embeddings : 154)    \n",
      "[SUCCESS] successfully created embeddings for company\n",
      "\n",
      "[START] starting work on death...\n",
      "[FLW] data for (fake) death successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for death  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) death successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for death  (lost embeddings : 11)    \n",
      "[SUCCESS] successfully created embeddings for death\n",
      "\n",
      "[START] starting work on gun...\n",
      "[FLW] data for (fake) gun successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for gun  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) gun successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for gun  (lost embeddings : 4)    \n",
      "[SUCCESS] successfully created embeddings for gun\n",
      "\n",
      "[START] starting work on id...\n",
      "[FLW] data for (fake) id successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for id  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) id successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for id  (lost embeddings : 14)    \n",
      "[SUCCESS] successfully created embeddings for id\n",
      "\n",
      "[START] starting work on interview...\n",
      "[FLW] data for (fake) interview successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for interview  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) interview successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for interview  (lost embeddings : 0)    \n",
      "[SUCCESS] successfully created embeddings for interview\n",
      "\n",
      "[START] starting work on passport...\n",
      "[FLW] data for (fake) passport successfully prepared. \n",
      "[FLW] Embeddings (FAKE and fake NOUN) constructed for passport  (lost embeddings : 0)    \n",
      "[FLW] data for (not fake) passport successfully prepared. \n",
      "[FLW] Embeddings (not fake NOUN) constructed for passport  (lost embeddings : 8)    \n",
      "[SUCCESS] successfully created embeddings for passport\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for noun in listNouns:\n",
    "    embeddings(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
